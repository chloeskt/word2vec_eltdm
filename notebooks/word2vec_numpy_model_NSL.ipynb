{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b083d0bf-d23a-45b1-b40c-ab0e765b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import pickle \n",
    "\n",
    "sys.path.append('/home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a8d855-3872-4774-bb10-ac73d5a02a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_eltdm.word2vec_numpy import (\n",
    "    Tokenizer, VocabCreator, DataLoader, TokenCleaner, Preprocessor,\n",
    "    Subsampler, SimpleWord2Vec, Optimizer, CrossEntropy, train, validate, NegWord2Vec,\n",
    "    NegativeSamplingLoss, OptimizeNSL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f517c-5602-449e-af4d-c881cff6c7c9",
   "metadata": {},
   "source": [
    "## Get data and create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db16e016-0677-4910-b630-53729efafd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/text8.txt\"\n",
    "\n",
    "RATIO = 1\n",
    "return_only_train = True\n",
    "tokenizer = Tokenizer(datapath)\n",
    "token_cleaner = TokenCleaner(freq_threshold=5)\n",
    "vocab_creator = VocabCreator()\n",
    "text8_dataset = Preprocessor(tokenizer, token_cleaner, vocab_creator, RATIO, return_only_train).preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5aaebb-65ad-4424-a030-42b5323710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 63492\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d804d6-3844-40d9-ad5b-21a6e5aef46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our train dataset: 10566033\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6be05b-fc5a-4b24-acca-85b56190cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our val dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our val dataset:\", len(text8_dataset.val_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285ebad4-5d79-486a-80b5-7e46c00f9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our test dataset:\", len(text8_dataset.test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b10ad-ecaa-432b-9cb7-78b685b22475",
   "metadata": {},
   "source": [
    "## Subsampling of frequent words, as in Mikolov 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f34449-b003-4adf-a869-11054cfe2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsampler = Subsampler(text8_dataset.train_tokens)\n",
    "text8_dataset.train_tokens, text8_dataset.frequencies = subsampler.subsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f0e1cf-47fb-4d76-a33b-af57574af026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary after subsampling of frequent words, for train: 63492\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary after subsampling of frequent words, for train:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44184e2-52b4-4063-8d19-d6efbb7139a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train dataset: 3767955\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007df8b-5765-4aff-9098-cf931f2a00d9",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b72de46-a391-4740-8a84-daa0a79e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(text8_dataset, text8_dataset.train_tokens, window, batch_size)\n",
    "\n",
    "# No need for validation and test sets\n",
    "# val_dataloader = DataLoader(text8_dataset, text8_dataset.val_tokens, window, batch_size)\n",
    "# test_dataloader = DataLoader(text8_dataset, text8_dataset.test_tokens, window, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01627b9b-7c44-41fb-a823-826ac32fc632",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbafcb7-5117-425e-8308-4cf8d06f505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the parameters\n",
    "len_vocab = len(text8_dataset.tokens_to_id)\n",
    "hidden_size = 500\n",
    "embedding_size = 300\n",
    "learning_rate = 1e-3\n",
    "n_samples = 5\n",
    "BEST_VAL_LOSS = 5.86\n",
    "\n",
    "# Get our noise distribution\n",
    "word_freqs = np.array(sorted(text8_dataset.frequencies.values(), reverse=True))\n",
    "unigram_dist = word_freqs / word_freqs.sum()\n",
    "noise_dist = unigram_dist ** (0.75) / np.sum(unigram_dist ** (0.75))\n",
    "\n",
    "# instantiate the model\n",
    "model = NegWord2Vec(\n",
    "    len_vocab,\n",
    "    hidden_size,\n",
    "    embedding_size,\n",
    "    noise_dist=noise_dist,\n",
    "    best_val_loss=BEST_VAL_LOSS\n",
    ")\n",
    "model.initialize_weights()\n",
    "\n",
    "# Load previous model\n",
    "\n",
    "#with open(\"../models/NegWord2Vec_5.869696641951837.p\", \"rb\") as file:\n",
    "#    model = pickle.load(file)\n",
    "#model = model[\"NegWord2Vec\"]\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = OptimizeNSL(model,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561f68-f821-4b25-abe5-a03a12b7eeba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3010db-91d0-4f91-8321-402245b16aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, criterion, optimizer):\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        model.train()\n",
    "        X, y = batch[\"X\"], batch[\"Y\"]\n",
    "        h = model.forward_input(X)\n",
    "        u = model.forward_output(y)\n",
    "        noise_vector = model.forward_noise(X.shape[1], n_samples)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss, grad_W1, grad_W2 = criterion(model, h, u, noise_vector, y)\n",
    "        optimizer.step(grad_W1, grad_W2)\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(\n",
    "                \"Current Training Loss {:.6}\".format(loss)\n",
    "            )\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def update_best_loss(model, val_loss):\n",
    "    # Update the model and best loss if we see improvements.\n",
    "    if not model.best_val_loss or val_loss < model.best_val_loss:\n",
    "        model.best_val_loss = val_loss\n",
    "        model.best_W1 = deepcopy(model.W1)\n",
    "        model.best_W2 = deepcopy(model.W2)\n",
    "        print(f\"Now best model has {val_loss} loss\")\n",
    "        model.save_model()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        X, y = batch[\"X\"], batch[\"Y\"]\n",
    "        h = model.forward_input(X)\n",
    "        u = model.forward_output(y)\n",
    "        noise_vector = model.forward_noise(X.shape[1], n_samples)\n",
    "        \n",
    "        loss, _, _ = criterion(model, h, u, noise_vector, y)\n",
    "        validation_loss += loss\n",
    "\n",
    "    validation_loss /= len(dataloader)\n",
    "\n",
    "    # Keep track of the best model\n",
    "    update_best_loss(model, validation_loss)\n",
    "\n",
    "    print(\"Validation Loss: \", validation_loss)\n",
    "\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422f48a-b406-4aba-8d17-504836ae2ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedca05ed667437ca345df5dae7bac2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### EPOCH 0 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bae6ea6bbb44c698d903ee62ea9282d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Training Loss 14.251\n"
     ]
    }
   ],
   "source": [
    "# train for some number of epochs\n",
    "epochs = 50\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "tbar = trange(epochs)\n",
    "for epoch in tbar:\n",
    "    print(f\"###################### EPOCH {epoch} ###########################\")\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    update_best_loss(model, train_loss)\n",
    "    \n",
    "    # No need for validation \n",
    "    \n",
    "    #val_loss_history.append(val_loss)\n",
    "    #val_loss = validate(model, val_dataloader, criterion)\n",
    "    #print(\"Last validation loss:\", val_loss)\n",
    "    #val_loss_history.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb051-d9e0-401c-94cd-7e79586a2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8106f89-8ea9-4a03-a515-c05610a383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571450f-b424-4a43-8890-7a83c57efbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df57d18-46c2-47db-8153-a8bbca04113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3202a7c-73d3-4c00-a2b0-4dcef4dd4b7b",
   "metadata": {},
   "source": [
    "## Results on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824177c1-0290-4e5a-99e7-9c04b0a9f429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
