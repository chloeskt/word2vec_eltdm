{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b083d0bf-d23a-45b1-b40c-ab0e765b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import pickle \n",
    "\n",
    "sys.path.append('/home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a8d855-3872-4774-bb10-ac73d5a02a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_eltdm.word2vec_numpy import (\n",
    "    Tokenizer, VocabCreator, DataLoader, TokenCleaner, Preprocessor,\n",
    "    Subsampler, SimpleWord2Vec, Optimizer, CrossEntropy, NegWord2Vec,\n",
    "    NegativeSamplingLoss, OptimizeNSL, evaluate, visualization_tsne, train_NSL,\n",
    "    update_best_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f517c-5602-449e-af4d-c881cff6c7c9",
   "metadata": {},
   "source": [
    "## Get data and create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db16e016-0677-4910-b630-53729efafd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/text8.txt\"\n",
    "\n",
    "RATIO = 1\n",
    "return_only_train = True\n",
    "tokenizer = Tokenizer(datapath)\n",
    "token_cleaner = TokenCleaner(freq_threshold=5)\n",
    "vocab_creator = VocabCreator()\n",
    "text8_dataset = Preprocessor(tokenizer, token_cleaner, vocab_creator, RATIO, return_only_train).preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5aaebb-65ad-4424-a030-42b5323710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 63492\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d804d6-3844-40d9-ad5b-21a6e5aef46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our train dataset: 10566033\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6be05b-fc5a-4b24-acca-85b56190cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our val dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our val dataset:\", len(text8_dataset.val_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285ebad4-5d79-486a-80b5-7e46c00f9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our test dataset:\", len(text8_dataset.test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b10ad-ecaa-432b-9cb7-78b685b22475",
   "metadata": {},
   "source": [
    "## Subsampling of frequent words, as in Mikolov 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f34449-b003-4adf-a869-11054cfe2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsampler = Subsampler(text8_dataset.train_tokens)\n",
    "text8_dataset.train_tokens, text8_dataset.frequencies = subsampler.subsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f0e1cf-47fb-4d76-a33b-af57574af026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary after subsampling of frequent words, for train: 63492\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary after subsampling of frequent words, for train:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44184e2-52b4-4063-8d19-d6efbb7139a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train dataset: 3767955\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007df8b-5765-4aff-9098-cf931f2a00d9",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b72de46-a391-4740-8a84-daa0a79e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(text8_dataset, text8_dataset.train_tokens, window, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01627b9b-7c44-41fb-a823-826ac32fc632",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbafcb7-5117-425e-8308-4cf8d06f505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the parameters\n",
    "len_vocab = len(text8_dataset.tokens_to_id)\n",
    "embedding_size = 300\n",
    "learning_rate = 3e-3 # 5e-2\n",
    "n_samples = 5\n",
    "BEST_VAL_LOSS = 2.83\n",
    "epochs = 10\n",
    "\n",
    "# hyperparameters for optimizer\n",
    "decay_rate = learning_rate / epochs\n",
    "method = \"none\" # or \"none\", \"exp_decay\", \"step_decay\", \"time_based\"\n",
    "\n",
    "# Get our noise distribution\n",
    "word_freqs = np.array(sorted(text8_dataset.frequencies.values(), reverse=True))\n",
    "unigram_dist = word_freqs / word_freqs.sum()\n",
    "noise_dist = unigram_dist ** (0.75) / np.sum(unigram_dist ** (0.75))\n",
    "\n",
    "# instantiate the model\n",
    "model = NegWord2Vec(\n",
    "    len_vocab,\n",
    "    embedding_size,\n",
    "    noise_dist=noise_dist,\n",
    "    best_val_loss=BEST_VAL_LOSS\n",
    ")\n",
    "model.initialize_weights()\n",
    "\n",
    "# Load previous model\n",
    "\n",
    "#with open(\"../word2vec_eltdm/models/NegWord2Vec_4.743055158391199.p\", \"rb\") as file:\n",
    "#    model = pickle.load(file)\n",
    "#model = model[\"NegWord2Vec\"]\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = OptimizeNSL(model,learning_rate, decay_rate, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561f68-f821-4b25-abe5-a03a12b7eeba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0422f48a-b406-4aba-8d17-504836ae2ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332781d5bfdb4d67a94b6bb6d47af1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### EPOCH 0 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9b3f2dcf8e49239bf50dc6fe4fc12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.1590862837596445\n",
      "year | christi, smuggling, wollstonecraft, shooters, robin\n",
      "x | transpired, sudden, sdr, strata, airs\n",
      "external | resolution, comets, racks, starships, elsa\n",
      "century | chos, testimonium, internationalized, condensing, gametophyte\n",
      "links | discos, wife, lethal, outfielder, adsorption\n",
      "english | yasunori, observer, throats, nborn, systematics\n",
      "would | two, washington, tend, zero, mesaoria\n",
      "states | desolation, slopes, toyota, carey, christiaan\n",
      "united | tempo, invasive, inferior, eight, schmitz\n",
      "series | wray, vernor, droit, dysprosium, barnum\n",
      "remaining | andhra, kempe, mifepristone, kalevala, incidences\n",
      "reached | motorway, hydrozoa, quoting, serco, evan\n",
      "animal | singhasari, videos, anderssen, hogg, dumbarton\n",
      "billion | gaited, rejection, relapses, bbc, sociologists\n",
      "signed | nec, megali, blockades, volcanism, amazing\n",
      "individuals | referendum, bahmani, clitoris, randle, hydroponic\n",
      "hebrew | dpp, scatter, started, michio, reverses\n",
      "location | deftones, coordinates, othon, angle, holley\n",
      "table | denton, momentary, bosch, taro, neg\n",
      "labour | manitoba, monro, cowboy, displayed, reflecting\n",
      "###################### EPOCH 1 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c368203837b4f1c844c12c79e2a37a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.085461579720116\n",
      "century | zero, two, six, four, nine\n",
      "would | two, zero, one, five, six\n",
      "example | zero, two, three, one, seven\n",
      "english | zero, four, six, one, two\n",
      "states | one, zero, four, nine, three\n",
      "known | five, zero, seven, three, one\n",
      "like | zero, one, five, three, four\n",
      "different | seven, zero, five, one, eight\n",
      "much | one, two, zero, seven, five\n",
      "war | zero, seven, also, one, six\n",
      "wars | zero, three, one, seven, two\n",
      "computers | nine, king, five, second, zero\n",
      "medicine | smaller, usually, within, nine, h\n",
      "motion | three, four, one, set, according\n",
      "translation | often, may, even, five, would\n",
      "supported | two, seven, three, one, called\n",
      "martin | five, zero, many, one, six\n",
      "users | zero, nine, though, six, first\n",
      "electric | house, one, two, published, considered\n",
      "bridge | one, would, popular, zero, often\n",
      "###################### EPOCH 2 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ad953f641e46efb7bfa366a1195dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.614056432495379\n",
      "history | made, include, modern, series, non\n",
      "early | external, american, important, new, part\n",
      "language | known, said, time, r, north\n",
      "different | e, left, features, seven, way\n",
      "also | r, war, william, known, another\n",
      "name | even, others, seen, long, zero\n",
      "well | following, many, groups, put, take\n",
      "since | often, four, american, much, number\n",
      "five | known, people, e, following, human\n",
      "see | many, death, later, found, named\n",
      "lack | example, political, r, external, time\n",
      "billion | long, others, found, many, th\n",
      "regarded | called, included, come, several, period\n",
      "table | often, see, time, system, associated\n",
      "internal | even, article, life, important, recent\n",
      "response | part, external, would, known, also\n",
      "hebrew | started, even, seven, home, return\n",
      "composer | french, make, less, two, five\n",
      "location | history, greek, even, may, six\n",
      "translation | even, may, often, term, french\n",
      "###################### EPOCH 3 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5e6e2b22942ddb69a83892e52138a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.180150290967018\n",
      "time | river, range, meaning, subsequent, authorities\n",
      "several | know, published, children, fit, administrative\n",
      "would | al, serious, friend, washington, bill\n",
      "see | across, administrative, eye, categories, identical\n",
      "based | al, talk, experimental, well, queen\n",
      "may | temporary, civilization, steve, distinct, record\n",
      "country | speak, austrian, initiative, vi, helps\n",
      "list | visible, guide, reversed, persian, players\n",
      "also | destruction, von, favor, detailed, chemistry\n",
      "united | club, india, identified, acquired, founding\n",
      "clear | air, evolved, novelist, marriage, aside\n",
      "round | passing, block, might, finished, preceding\n",
      "computers | distribution, linear, sweet, collaboration, king\n",
      "province | sir, carl, le, tv, el\n",
      "ship | sources, gradually, dry, competition, evidence\n",
      "medicine | theme, rule, invented, el, smaller\n",
      "daughter | talent, expanded, roots, author, bc\n",
      "speech | contrary, collectively, dry, reference, run\n",
      "environment | honor, novels, russian, least, book\n",
      "beyond | laboratory, merely, consequences, film, seen\n",
      "###################### EPOCH 4 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6199d3e5d027433280aa04fd9ea30186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.0162638267432844\n",
      "states | slopes, river, carey, converting, demise\n",
      "language | standing, camps, festival, visited, joy\n",
      "year | smuggling, artistic, christi, hero, robin\n",
      "de | intensity, occupied, henri, witnessed, penguin\n",
      "new | rites, resurrection, scene, fiber, se\n",
      "system | periodic, nose, embassy, associates, referendum\n",
      "high | proclaiming, dr, terminated, shores, herd\n",
      "number | hunter, sick, engraving, sizes, commonly\n",
      "state | bishops, denounced, kent, successors, fearing\n",
      "years | caution, beta, chemical, ottawa, starts\n",
      "appointed | display, triple, dreams, designated, intensely\n",
      "reading | object, resource, parameter, shepherd, bishop\n",
      "supported | terrible, gases, mp, gear, franklin\n",
      "consider | england, quit, heated, laureate, actress\n",
      "conflict | sir, immense, bottom, hang, suffix\n",
      "martin | decisive, dr, satellites, mr, joseph\n",
      "environment | novels, rescue, honor, assets, dioxide\n",
      "sexual | images, degree, gustave, du, smell\n",
      "beyond | macmillan, consequences, become, laboratory, circuit\n",
      "places | unitary, loved, response, stands, genuine\n",
      "###################### EPOCH 5 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b3f86f5b6d471ca686e964cd1a8a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.9416344074863687\n",
      "common | militias, accent, sino, solstice, downloaded\n",
      "c | right, carol, proximity, lots, beverly\n",
      "american | coastline, masterpiece, comprises, eventual, nottingham\n",
      "would | rape, found, serious, charm, atlantis\n",
      "state | bishops, tacitus, coincide, fearing, denounced\n",
      "f | attained, dar, console, eventual, fuller\n",
      "de | intensity, frankish, curb, occupied, henri\n",
      "eight | vested, macintosh, revolutionary, violinist, moderate\n",
      "english | observer, estimation, query, mercy, agents\n",
      "first | ghost, valuable, cited, marcel, corresponded\n",
      "temperature | purchases, sees, guided, orthodox, absorption\n",
      "sold | beaten, employment, fruits, propaganda, trajectory\n",
      "sexual | images, secondly, degree, gustave, smell\n",
      "committee | firstly, mellon, bald, heaven, easter\n",
      "motion | indiana, natalie, grades, simplicity, thriller\n",
      "consider | england, quit, assorted, dining, peruvian\n",
      "ed | consistency, minimize, princes, directing, precede\n",
      "station | lord, heated, neal, rhythm, slashdot\n",
      "martin | decisive, satellites, bundled, basil, culinary\n",
      "acts | sleep, communicated, psychologist, fa, propose\n",
      "###################### EPOCH 6 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bee63f556c746efaf09e256e12ab0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.901502042916605\n",
      "south | uttered, richter, immersion, dod, evangelicalism\n",
      "game | handicrafts, drought, authorizing, put, interstellar\n",
      "zero | many, native, political, eating, including\n",
      "although | mess, pink, hemingway, steadfast, taipei\n",
      "n | gunfire, really, filmmaking, busiest, youthful\n",
      "modern | critique, analyst, slash, fermentation, habitats\n",
      "new | rites, scene, resurrection, palmer, index\n",
      "number | engraving, lawn, commonly, residue, fertilized\n",
      "links | outfielder, pornographic, myanmar, vedic, swallow\n",
      "american | coastline, nottingham, masterpiece, comprises, oats\n",
      "calendar | prompting, cm, nigeria, kafka, empiricism\n",
      "ed | consistency, precede, minimize, princes, cigarette\n",
      "sold | umpires, beaten, josh, trajectory, lung\n",
      "conflict | mez, hang, titanium, parry, immense\n",
      "kind | aq, send, frankish, fats, adequately\n",
      "users | retiring, uv, westminster, xiii, gottfried\n",
      "purpose | intellectuals, extraordinary, rival, leads, fuselage\n",
      "carbon | meiji, girolamo, tilted, cock, tactical\n",
      "intended | precession, simplistic, swinging, cheney, jennifer\n",
      "evolution | piercing, brighter, respiratory, infections, diffuse\n",
      "###################### EPOCH 7 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca58b6f17be94d7f823f88d86b2dbdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8730/3680915541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"###################### EPOCH {epoch} ###########################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_NSL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/utils.py\u001b[0m in \u001b[0;36mtrain_NSL\u001b[0;34m(model, train_dataloader, criterion, optimizer, n_samples)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# negative sampling loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         loss, grad_W1, grad_W2_pos, grad_W2_neg = criterion(\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model, input_vectors, output_vectors, noise_vectors, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         grad1, grad2_pos, grad2_neg = self.backward(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, model, input_vectors, output_vectors, noise_vectors, y)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mcontext_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mcontext_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_noise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# gradient wrt W1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ENSAE/elements_logiciels/word2vec_eltdm/env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     47\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train for some number of epochs\n",
    "train_loss_history = []\n",
    "tbar = trange(epochs)\n",
    "\n",
    "for epoch in tbar:\n",
    "    print(f\"###################### EPOCH {epoch} ###########################\")\n",
    "    \n",
    "    train_loss = train_NSL(model, train_dataloader, criterion, optimizer, n_samples)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    update_best_loss(model, train_loss)\n",
    "    \n",
    "    # update learning rate \n",
    "    optimizer.update_lr(epoch)\n",
    "    \n",
    "    embeddings = model.W1\n",
    "    evaluate(embeddings, text8_dataset.id_to_tokens, nb_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8801553-e637-4c90-83e7-2186d50045ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second | phonemic, known, yeast, pitched, cortex\n",
      "e | faithfulness, textile, force, wide, meditating\n",
      "form | tended, force, finding, jars, receipt\n",
      "history | spit, extradition, unofficially, adolescent, romances\n",
      "many | zero, see, well, three, first\n",
      "state | influence, tacitus, fearing, bishops, polite\n",
      "five | yards, people, known, procedures, ut\n",
      "different | tl, deformed, rfc, utrecht, rejecting\n",
      "common | militias, solstice, liszt, counteract, sino\n",
      "eight | every, vested, zero, revolutionary, salinity\n",
      "environment | dive, chasing, dioxide, honor, assets\n",
      "noted | testify, conspiring, sensations, spending, unfavorable\n",
      "temperature | shlomo, purchases, tokugawa, boehm, vicar\n",
      "performed | verona, white, asimov, repeat, calculus\n",
      "values | lemma, kt, extravagant, affiliations, surveyor\n",
      "wars | vegetarian, elders, receives, deputies, avec\n",
      "miles | owed, nig, frankie, blondie, canine\n",
      "approximately | proportions, militant, appended, presidential, shinto\n",
      "province | conspicuous, tooth, qui, informs, stem\n",
      "speaking | surname, extensively, rhodes, spends, bailey\n"
     ]
    }
   ],
   "source": [
    "evaluate(embeddings, text8_dataset.id_to_tokens, nb_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df57d18-46c2-47db-8153-a8bbca04113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3202a7c-73d3-4c00-a2b0-4dcef4dd4b7b",
   "metadata": {},
   "source": [
    "## Evaluation on the task of word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e336c-c492-4a8c-b27e-0c531fbafcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../word2vec_eltdm/models/NegWord2Vec_4.753405412967128.p\"\n",
    "with open(filepath, \"rb\") as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824177c1-0290-4e5a-99e7-9c04b0a9f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model[\"NegWord2Vec\"]\n",
    "embeddings = model.best_W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da161421-06d5-4594-93c2-bbba8fa4bac2",
   "metadata": {},
   "source": [
    "### Evaluate using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e693c8-2109-4540-a27d-a7645776e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(embeddings, text8_dataset.id_to_tokens, nb_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab0a93-472d-43fb-b46c-d3d6a2dd1f11",
   "metadata": {},
   "source": [
    "### t-SNE embedding visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167a904-ba0a-4a4c-afe4-010eafbe667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_tsne(embeddings, text8_dataset.id_to_tokens, nb_words = 400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
