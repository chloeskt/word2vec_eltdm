{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b083d0bf-d23a-45b1-b40c-ab0e765b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import pickle \n",
    "\n",
    "sys.path.append('/home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a8d855-3872-4774-bb10-ac73d5a02a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_eltdm.word2vec_numpy import (\n",
    "    Tokenizer, VocabCreator, DataLoader, TokenCleaner, Preprocessor,\n",
    "    Subsampler, SimpleWord2Vec, Optimizer, CrossEntropy, NegWord2Vec,\n",
    "    NegativeSamplingLoss, OptimizeNSL, evaluate, visualization_tsne, train_NSL,\n",
    "    update_best_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f517c-5602-449e-af4d-c881cff6c7c9",
   "metadata": {},
   "source": [
    "## Get data and create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db16e016-0677-4910-b630-53729efafd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/text8.txt\"\n",
    "\n",
    "RATIO = 1\n",
    "return_only_train = True\n",
    "tokenizer = Tokenizer(datapath)\n",
    "token_cleaner = TokenCleaner(freq_threshold=5)\n",
    "vocab_creator = VocabCreator()\n",
    "text8_dataset = Preprocessor(tokenizer, token_cleaner, vocab_creator, RATIO, return_only_train).preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5aaebb-65ad-4424-a030-42b5323710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 63492\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d804d6-3844-40d9-ad5b-21a6e5aef46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our train dataset: 10566033\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6be05b-fc5a-4b24-acca-85b56190cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our val dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our val dataset:\", len(text8_dataset.val_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285ebad4-5d79-486a-80b5-7e46c00f9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our test dataset:\", len(text8_dataset.test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b10ad-ecaa-432b-9cb7-78b685b22475",
   "metadata": {},
   "source": [
    "## Subsampling of frequent words, as in Mikolov 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f34449-b003-4adf-a869-11054cfe2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsampler = Subsampler(text8_dataset.train_tokens)\n",
    "text8_dataset.train_tokens, text8_dataset.frequencies = subsampler.subsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f0e1cf-47fb-4d76-a33b-af57574af026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary after subsampling of frequent words, for train: 63492\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary after subsampling of frequent words, for train:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44184e2-52b4-4063-8d19-d6efbb7139a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train dataset: 3767955\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007df8b-5765-4aff-9098-cf931f2a00d9",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b72de46-a391-4740-8a84-daa0a79e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(text8_dataset, text8_dataset.train_tokens, window, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01627b9b-7c44-41fb-a823-826ac32fc632",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbafcb7-5117-425e-8308-4cf8d06f505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the parameters\n",
    "len_vocab = len(text8_dataset.tokens_to_id)\n",
    "embedding_size = 300\n",
    "learning_rate = 5e-2\n",
    "n_samples = 5\n",
    "BEST_VAL_LOSS = 2.84\n",
    "epochs = 10\n",
    "\n",
    "# hyperparameters for optimizer\n",
    "decay_rate = learning_rate / epochs\n",
    "method = \"time_based\" # or \"none\", \"exp_decay\", \"step_decay\", \"time_based\"\n",
    "\n",
    "# Get our noise distribution\n",
    "word_freqs = np.array(sorted(text8_dataset.frequencies.values(), reverse=True))\n",
    "unigram_dist = word_freqs / word_freqs.sum()\n",
    "noise_dist = unigram_dist ** (0.75) / np.sum(unigram_dist ** (0.75))\n",
    "\n",
    "# instantiate the model\n",
    "model = NegWord2Vec(\n",
    "    len_vocab,\n",
    "    embedding_size,\n",
    "    noise_dist=noise_dist,\n",
    "    best_val_loss=BEST_VAL_LOSS\n",
    ")\n",
    "model.initialize_weights()\n",
    "\n",
    "# Load previous model\n",
    "\n",
    "#with open(\"../word2vec_eltdm/models/NegWord2Vec_4.743055158391199.p\", \"rb\") as file:\n",
    "#    model = pickle.load(file)\n",
    "#model = model[\"NegWord2Vec\"]\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = OptimizeNSL(model,learning_rate, decay_rate, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561f68-f821-4b25-abe5-a03a12b7eeba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422f48a-b406-4aba-8d17-504836ae2ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb1a4c65cf4763b31e81133ac497c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### EPOCH 0 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192e4e16b197462eaf9010c44d93b856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train for some number of epochs\n",
    "train_loss_history = []\n",
    "tbar = trange(epochs)\n",
    "\n",
    "for epoch in tbar:\n",
    "    print(f\"###################### EPOCH {epoch} ###########################\")\n",
    "    \n",
    "    train_loss = train_NSL(model, train_dataloader, criterion, optimizer, n_samples)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    update_best_loss(model, train_loss)\n",
    "    \n",
    "    # update learning rate \n",
    "    optimizer.update_lr(epoch)\n",
    "    \n",
    "    embeddings = model.W1\n",
    "    evaluate(embeddings, text8_dataset.id_to_tokens, nb_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df57d18-46c2-47db-8153-a8bbca04113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3202a7c-73d3-4c00-a2b0-4dcef4dd4b7b",
   "metadata": {},
   "source": [
    "## Evaluation on the task of word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e336c-c492-4a8c-b27e-0c531fbafcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../word2vec_eltdm/models/NegWord2Vec_4.753405412967128.p\"\n",
    "with open(filepath, \"rb\") as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824177c1-0290-4e5a-99e7-9c04b0a9f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model[\"NegWord2Vec\"]\n",
    "embeddings = model.best_W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da161421-06d5-4594-93c2-bbba8fa4bac2",
   "metadata": {},
   "source": [
    "### Evaluate using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e693c8-2109-4540-a27d-a7645776e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(embeddings, text8_dataset.id_to_tokens, nb_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab0a93-472d-43fb-b46c-d3d6a2dd1f11",
   "metadata": {},
   "source": [
    "### t-SNE embedding visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167a904-ba0a-4a4c-afe4-010eafbe667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_tsne(embeddings, text8_dataset.id_to_tokens, nb_words = 400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
