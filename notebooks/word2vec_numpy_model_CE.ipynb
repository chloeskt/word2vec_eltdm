{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b083d0bf-d23a-45b1-b40c-ab0e765b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a8d855-3872-4774-bb10-ac73d5a02a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_eltdm.common import Tokenizer, VocabCreator, DataLoader, TokenCleaner, Preprocessor, Subsampler\n",
    "from word2vec_eltdm.word2vec_numpy import (\n",
    "    SimpleWord2Vec, Optimizer, CrossEntropy, train_default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f517c-5602-449e-af4d-c881cff6c7c9",
   "metadata": {},
   "source": [
    "## Get data and create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db16e016-0677-4910-b630-53729efafd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/text8.txt\"\n",
    "\n",
    "RATIO = 0.2\n",
    "tokenizer = Tokenizer(datapath)\n",
    "token_cleaner = TokenCleaner(freq_threshold=5)\n",
    "vocab_creator = VocabCreator()\n",
    "text8_dataset = Preprocessor(tokenizer, token_cleaner, vocab_creator, RATIO).preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5aaebb-65ad-4424-a030-42b5323710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 25876\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d804d6-3844-40d9-ad5b-21a6e5aef46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our train dataset: 2029882\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6be05b-fc5a-4b24-acca-85b56190cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our val dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our val dataset:\", len(text8_dataset.val_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285ebad4-5d79-486a-80b5-7e46c00f9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our test dataset:\", len(text8_dataset.test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b10ad-ecaa-432b-9cb7-78b685b22475",
   "metadata": {},
   "source": [
    "## Subsampling of frequent words, as in Mikolov 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f34449-b003-4adf-a869-11054cfe2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsampler = Subsampler(text8_dataset.train_tokens)\n",
    "text8_dataset.train_tokens, text8_dataset.frequencies = subsampler.subsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f0e1cf-47fb-4d76-a33b-af57574af026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary after subsampling of frequent words, for train: 25876\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary after subsampling of frequent words, for train:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44184e2-52b4-4063-8d19-d6efbb7139a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train dataset: 652544\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007df8b-5765-4aff-9098-cf931f2a00d9",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b72de46-a391-4740-8a84-daa0a79e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(text8_dataset, text8_dataset.train_tokens, window, batch_size)\n",
    "val_dataloader = DataLoader(text8_dataset, text8_dataset.val_tokens, window, batch_size)\n",
    "test_dataloader = DataLoader(text8_dataset, text8_dataset.test_tokens, window, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01627b9b-7c44-41fb-a823-826ac32fc632",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dbafcb7-5117-425e-8308-4cf8d06f505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the parameters\n",
    "len_vocab = len(text8_dataset.tokens_to_id)\n",
    "embedding_size = 300\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# instantiate the model\n",
    "model = SimpleWord2Vec(\n",
    "    len_vocab,\n",
    "    embedding_size\n",
    ")\n",
    "model.initialize_weights()\n",
    "#model.initialize_embeddings()\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = CrossEntropy() \n",
    "optimizer = Optimizer(model,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561f68-f821-4b25-abe5-a03a12b7eeba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0422f48a-b406-4aba-8d17-504836ae2ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b490b9081add44a3bed25250b2c2c050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### EPOCH 0 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec2b274ab8c40e8a8392a56329327bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Training Loss 6.86976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4152/1651291040.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"###################### EPOCH {epoch} ###########################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/utils.py\u001b[0m in \u001b[0;36mtrain_default\u001b[0;34m(model, train_dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/models.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_softmax)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mdW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train for some number of epochs\n",
    "epochs = 5\n",
    "train_loss_history = []\n",
    "tbar = trange(epochs)\n",
    "for epoch in tbar:\n",
    "    print(f\"###################### EPOCH {epoch} ###########################\")\n",
    "    train_loss = train_default(model, train_dataloader, criterion, optimizer)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    #if epoch % 2 == 0:\n",
    "    #    validation_loss = validate(model, val_dataloader, criterion)\n",
    "    #    print(\"Validation loss:\", validation_loss)\n",
    "    #    val_loss_history.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb051-d9e0-401c-94cd-7e79586a2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8106f89-8ea9-4a03-a515-c05610a383da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0114704   0.00781689 -0.07592398 ... -0.06584696 -0.01126968\n",
      "  -0.03894035]\n",
      " [-0.04459164 -0.00029665 -0.06110165 ...  0.00161734  0.0517953\n",
      "   0.03251175]\n",
      " [ 0.01919322  0.07551723 -0.02297194 ...  0.06101599  0.02081512\n",
      "   0.05297369]\n",
      " ...\n",
      " [ 0.07640455 -0.08440605  0.04377483 ...  0.06914157  0.08793141\n",
      "   0.07489028]\n",
      " [ 0.05681877 -0.00815245 -0.06372532 ... -0.0342809  -0.0760821\n",
      "   0.06272569]\n",
      " [ 0.04764095  0.07825114  0.03601919 ... -0.07157029  0.02912176\n",
      "   0.08239927]]\n"
     ]
    }
   ],
   "source": [
    "print(model.best_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2571450f-b424-4a43-8890-7a83c57efbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01728752  0.06498035  0.03054626 ... -0.07648543  0.0535056\n",
      "   0.02259568]\n",
      " [ 0.08100971  0.08421894  0.04308239 ... -0.10965304 -0.00387338\n",
      "   0.04331428]\n",
      " [-0.06079111  0.0508666  -0.06100289 ...  0.01379212  0.00782755\n",
      "  -0.05316632]\n",
      " ...\n",
      " [ 0.03948089 -0.08665571  0.01979475 ... -0.06374977 -0.05353155\n",
      "   0.10724318]\n",
      " [ 0.01584454 -0.02193368  0.01995545 ... -0.03979566  0.00870762\n",
      "   0.03861215]\n",
      " [ 0.09548114 -0.05916092  0.04088983 ... -0.01783159 -0.04229519\n",
      "  -0.0209925 ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.best_W2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
