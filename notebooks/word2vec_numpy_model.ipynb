{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b083d0bf-d23a-45b1-b40c-ab0e765b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a8d855-3872-4774-bb10-ac73d5a02a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_eltdm.word2vec_numpy import (\n",
    "    Tokenizer, VocabCreator, DataLoader, TokenCleaner, Preprocessor,\n",
    "    Subsampler, SimpleWord2Vec, Optimizer, CrossEntropy, train, validate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f517c-5602-449e-af4d-c881cff6c7c9",
   "metadata": {},
   "source": [
    "## Get data and create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db16e016-0677-4910-b630-53729efafd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/text8.txt\"\n",
    "\n",
    "tokenizer = Tokenizer(datapath)\n",
    "token_cleaner = TokenCleaner(freq_threshold=3)\n",
    "vocab_creator = VocabCreator()\n",
    "text8_dataset = Preprocessor(tokenizer, token_cleaner, vocab_creator).preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5aaebb-65ad-4424-a030-42b5323710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 80383\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d804d6-3844-40d9-ad5b-21a6e5aef46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our train dataset: 7986226\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6be05b-fc5a-4b24-acca-85b56190cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our val dataset: 1331038\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our val dataset:\", len(text8_dataset.val_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285ebad4-5d79-486a-80b5-7e46c00f9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in our test dataset: 1331038\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in our test dataset:\", len(text8_dataset.test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b10ad-ecaa-432b-9cb7-78b685b22475",
   "metadata": {},
   "source": [
    "## Subsampling of frequent words, as in Mikolov 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f34449-b003-4adf-a869-11054cfe2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsampler = Subsampler(text8_dataset.train_tokens)\n",
    "text8_dataset.train_tokens = subsampler.subsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f0e1cf-47fb-4d76-a33b-af57574af026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary after subsampling of frequent words, for train: 80383\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our vocabulary after subsampling of frequent words, for train:\", len(text8_dataset.tokens_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44184e2-52b4-4063-8d19-d6efbb7139a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train dataset: 2893184\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in train dataset:\", len(text8_dataset.train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007df8b-5765-4aff-9098-cf931f2a00d9",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b72de46-a391-4740-8a84-daa0a79e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(text8_dataset, text8_dataset.train_tokens, window, batch_size)\n",
    "val_dataloader = DataLoader(text8_dataset, text8_dataset.val_tokens, window, batch_size)\n",
    "test_dataloader = DataLoader(text8_dataset, text8_dataset.test_tokens, window, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01627b9b-7c44-41fb-a823-826ac32fc632",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbafcb7-5117-425e-8308-4cf8d06f505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the parameters\n",
    "len_vocab = len(text8_dataset.tokens_to_id)\n",
    "num_layer = 1\n",
    "hidden_size = 500\n",
    "embedding_size = 300\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# instantiate the model\n",
    "model = SimpleWord2Vec(\n",
    "    len_vocab,\n",
    "    num_layer,\n",
    "    hidden_size,\n",
    "    embedding_size\n",
    ")\n",
    "model.initialize_weights()\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = CrossEntropy() \n",
    "optimizer = Optimizer(model,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561f68-f821-4b25-abe5-a03a12b7eeba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422f48a-b406-4aba-8d17-504836ae2ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df4bc76e7654fa8b8770d9110419749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### EPOCH 0 ###########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e227bfbd236448df8e598cf77aa47579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train for some number of epochs\n",
    "epochs = 15\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "tbar = trange(epochs)\n",
    "for epoch in tbar:\n",
    "    print(f\"###################### EPOCH {epoch} ###########################\")\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        validation_loss = validate(model, val_dataloader, criterion)\n",
    "        print(\"Validation loss:\", validation_loss)\n",
    "        val_loss_history.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb051-d9e0-401c-94cd-7e79586a2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8106f89-8ea9-4a03-a515-c05610a383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571450f-b424-4a43-8890-7a83c57efbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3202a7c-73d3-4c00-a2b0-4dcef4dd4b7b",
   "metadata": {},
   "source": [
    "## Results on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027cc83-a368-4c65-b962-5c0910977506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
