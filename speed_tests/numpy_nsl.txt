Timer unit: 1e-06 s

Total time: 0 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py
Function: forward at line 25

Total time: 3.73129 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py
Function: forward at line 51

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    51                                               @profile
    52                                               def forward(self, input_vectors, output_vectors, noise_vectors):
    53      1131        847.0      0.7      0.0          batch_size, embed_size = input_vectors.shape
    54
    55                                                   # Input vectors should be a batch of column vectors
    56      1131        829.0      0.7      0.0          input_vectors = input_vectors.reshape(batch_size, embed_size, 1)
    57
    58                                                   # Output vectors should be a batch of row vectors
    59      1131        574.0      0.5      0.0          output_vectors = output_vectors.reshape(batch_size, 1, embed_size)
    60
    61                                                   # correct log-sigmoid loss
    62      1131     436170.0    385.6     11.7          out_loss = np.log(self.sigmoid(output_vectors @ input_vectors))
    63      1131       1258.0      1.1      0.0          out_loss = out_loss.squeeze()
    64
    65                                                   # incorrect log-sigmoid loss
    66      1131    3224338.0   2850.9     86.4          noise_loss = np.log(self.sigmoid(-noise_vectors @ input_vectors))
    67      2262      34922.0     15.4      0.9          noise_loss = noise_loss.squeeze().sum(
    68      1131        329.0      0.3      0.0              1
    69                                                   )  # sum the losses over the sample of noise vectors
    70
    71                                                   # negate and sum correct and noisy log-sigmoid losses
    72                                                   # return average batch loss
    73      1131      32021.0     28.3      0.9          return -(out_loss + noise_loss).mean()

Total time: 12.7157 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py
Function: backward at line 80

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    80                                               @profile
    81                                               def backward(self, model, input_vectors, output_vectors, noise_vectors, y):
    82      1131        870.0      0.8      0.0          batch_size, embed_size = input_vectors.shape
    83
    84                                                   # Input vectors should be a batch of column vectors
    85      1131       1497.0      1.3      0.0          input_vectors = input_vectors.reshape(batch_size, embed_size, 1)
    86                                                   # Output vectors should be a batch of row vectors
    87      1131        629.0      0.6      0.0          output_vectors = output_vectors.reshape(batch_size, 1, embed_size)
    88
    89      1131       1038.0      0.9      0.0          sigmoid_context = (
    90      1131     398903.0    352.7      3.1              self.sigmoid(output_vectors @ input_vectors).squeeze(axis=2) - 1
    91                                                   ).squeeze()
    92      2262     386888.0    171.0      3.0          product_context = np.multiply(
    93      1131       2045.0      1.8      0.0              output_vectors.squeeze(), sigmoid_context[:, None]
    94                                                   )
    95
    96      1131    3096414.0   2737.8     24.4          sigmoid_noise = self.sigmoid(-noise_vectors @ input_vectors)
    97      1131      21970.0     19.4      0.2          sigmoid_noise -= np.ones(sigmoid_noise.shape)
    98
    99      1131    2193644.0   1939.6     17.3          context_noise = np.multiply(noise_vectors, sigmoid_noise)
   100      1131    1557354.0   1377.0     12.2          context_noise = context_noise.sum(axis=1)
   101
   102                                                   # gradient wrt W1
   103      1131     387336.0    342.5      3.0          grad_W1 = product_context - context_noise
   104
   105                                                   # gradient wrt context words
   106      2262     412315.0    182.3      3.2          grad_W2_positive = np.multiply(
   107      1131       2305.0      2.0      0.0              input_vectors.squeeze(), sigmoid_context[:, None]
   108                                                   )
   109
   110                                                   # gradient wrt negative words (one gradient for each negative word and for each
   111                                                   # context word)
   112      2262    2091078.0    924.4     16.4          grad_W2_negative = np.negative(
   113      1131    2160433.0   1910.2     17.0              np.multiply(input_vectors.reshape(batch_size, 1, embed_size), sigmoid_noise)
   114                                                   )
   115
   116      1131        998.0      0.9      0.0          return grad_W1, grad_W2_positive, grad_W2_negative

Total time: 0.261846 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/models.py
Function: forward_input at line 144

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   144                                               @profile
   145                                               def forward_input(self, X):
   146      1131     259939.0    229.8     99.3          h = self.W1[X.squeeze(), :]
   147      1131       1669.0      1.5      0.6          self.cache["X"] = X
   148      1131        238.0      0.2      0.1          return h

Total time: 0.271957 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/models.py
Function: forward_output at line 150

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   150                                               @profile
   151                                               def forward_output(self, y):
   152      1131     270731.0    239.4     99.5          u = self.W2[y.squeeze(), :]
   153      1131       1005.0      0.9      0.4          self.cache["y"] = y
   154      1131        221.0      0.2      0.1          return u

Total time: 2.58108 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/models.py
Function: forward_noise at line 156

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   156                                               @profile
   157                                               def forward_noise(self, batch_size, n_samples):
   158      1131        626.0      0.6      0.0          if self.noise_dist is None:
   159                                                       # Sample words uniformly
   160                                                       self.noise_dist = np.ones(self.len_vocab) / self.len_vocab
   161
   162                                                   # Sample words from our noise distribution
   163                                                   # Use torch multinomial because it has the behavior we want compared to np multinomial
   164      2262     614653.0    271.7     23.8          noise_words = torch.multinomial(
   165      1131      15066.0     13.3      0.6              torch.from_numpy(self.noise_dist), batch_size * n_samples, replacement=True
   166                                                   ).numpy()
   167
   168                                                   # Get the noise embeddings
   169      2262    1946619.0    860.6     75.4          noise_vector = self.W2[noise_words, :].reshape(
   170      1131        847.0      0.7      0.0              batch_size, n_samples, self.embedding_size
   171                                                   )
   172      1131       2529.0      2.2      0.1          self.cache["noise_words"] = noise_words
   173      1131        434.0      0.4      0.0          self.cache["n_samples"] = n_samples
   174
   175      1131        307.0      0.3      0.0          return noise_vector

Total time: 2e-06 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/optimizers.py
Function: update_lr at line 29

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    29                                               @profile
    30                                               def update_lr(self, epoch: int):
    31         1          1.0      1.0     50.0          if self.method == "time_based":
    32         1          1.0      1.0     50.0              self.learning_rate *= 1.0 / (1.0 + self.decay_rate * self.iterations)
    33
    34                                                   elif self.method == "exp_decay":
    35                                                       k = 0.001
    36                                                       self.learning_rate *= np.exp(-k * self.iterations)
    37
    38                                                   elif self.method == "step_decay":
    39                                                       drop = 0.5
    40                                                       epoch_drop = 5.0
    41                                                       if epoch % epoch_drop == 0 and epoch != 0:
    42                                                           self.learning_rate *= drop
    43
    44                                                   elif self.method == "none":
    45                                                       pass
    46
    47                                                   else:
    48                                                       raise NotImplementedError

Total time: 9.80496 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/optimizers.py
Function: step at line 83

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    83                                               @profile
    84                                               def step(self, dW1, dW2_pos, dW2_neg):
    85      1131       1110.0      1.0      0.0          batch_size, embed_size = dW1.shape
    86                                                   # update W1 weights
    87      1131       1284.0      1.1      0.0          X = self.model.cache["X"]
    88      1131    1099031.0    971.7     11.2          self.model.W1[X.squeeze(), :] -= self.learning_rate * dW1
    89
    90                                                   # update W2 weights for positive samples
    91      1131       1625.0      1.4      0.0          y = self.model.cache["y"]
    92      1131     839266.0    742.1      8.6          self.model.W2[y.squeeze(), :] -= self.learning_rate * dW2_pos
    93
    94                                                   # update W2 weights for negative samples
    95      2262       2761.0      1.2      0.0          dW2_neg = dW2_neg.reshape(
    96      1131        938.0      0.8      0.0              batch_size * self.model.cache["n_samples"], embed_size
    97                                                   )
    98      2262    5904462.0   2610.3     60.2          self.model.W2[self.model.cache["noise_words"], :] -= (
    99      1131    1952015.0   1725.9     19.9              self.learning_rate * dW2_neg
   100                                                   )
   101
   102      1131       2464.0      2.2      0.0          self.iterations += 1

Total time: 30.9357 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/utils.py
Function: train_NSL at line 58

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    58                                           @profile
    59                                           def train_NSL(
    60                                               model: NegWord2Vec,
    61                                               train_dataloader: DataLoader,
    62                                               criterion: NegativeSamplingLoss,
    63                                               optimizer: OptimizeNSL,
    64                                               n_samples: int,
    65                                           ) -> float:
    66         1          1.0      1.0      0.0      train_loss = 0.0
    67      1132    1436514.0   1269.0      4.6      for i, batch in enumerate(tqdm(train_dataloader)):
    68      1131       2654.0      2.3      0.0          model.train()
    69      1131        598.0      0.5      0.0          X, y = batch["X"], batch["Y"]
    70      1131     268436.0    237.3      0.9          h = model.forward_input(X)
    71      1131     277280.0    245.2      0.9          u = model.forward_output(y)
    72      1131    2614633.0   2311.8      8.5          noise_vector = model.forward_noise(X.shape[1], n_samples)
    73
    74                                                   # negative sampling loss
    75      2262   16512397.0   7299.9     53.4          loss, grad_W1, grad_W2_pos, grad_W2_neg = criterion(
    76      1131        435.0      0.4      0.0              model, h, u, noise_vector, y
    77                                                   )
    78      1131    9820992.0   8683.5     31.7          optimizer.step(grad_W1, grad_W2_pos, grad_W2_neg)
    79
    80      1131       1745.0      1.5      0.0          train_loss += loss
    81
    82         1          6.0      6.0      0.0      train_loss /= len(train_dataloader)
    83         1          1.0      1.0      0.0      return train_loss

Total time: 30.9428 s
File: speed_tests/numpy_nsl.py
Function: train_wrapper at line 67

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    67                                           @profile
    68                                           def train_wrapper(epochs, model, train_dataloader, criterion, optimizer, n_samples):
    69         2       1185.0    592.5      0.0      for epoch in tqdm(range(epochs)):
    70         1          5.0      5.0      0.0          print(f"###################### EPOCH {epoch} ###########################")
    71
    72         1   30941610.0 30941610.0    100.0          train_loss = train_NSL(model, train_dataloader, criterion, optimizer, n_samples)
    73         1         28.0     28.0      0.0          print("Training loss:", train_loss)
    74
    75                                                   # update learning rate
    76         1         22.0     22.0      0.0          optimizer.update_lr(epoch)
