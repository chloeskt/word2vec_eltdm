Timer unit: 1e-06 s

Total time: 0.0971 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py
Function: forward at line 25

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    25                                               @profile
    26                                               def forward(self, preds, y):
    27      1131       2303.0      2.0      2.4          m = preds.shape[1]
    28      2262      30030.0     13.3     30.9          return -(1 / m) * np.sum(
    29      1131      64767.0     57.3     66.7              np.log(preds[y.flatten(), np.arange(y.shape[1])] + 0.001)
    30                                                   )

Total time: 0.033117 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/losses.py
Function: backward at line 32

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    32                                               @profile
    33                                               def backward(self, preds, y):
    34      1131        853.0      0.8      2.6          m = preds.shape[1]
    35      1131      31504.0     27.9     95.1          preds[y.flatten(), np.arange(m)] -= 1.0
    36      1131        513.0      0.5      1.5          grad = preds
    37      1131        247.0      0.2      0.7          return grad

Total time: 514.208 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/models.py
Function: forward at line 55

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    55                                               @profile
    56                                               def forward(self, X):
    57      1131       1110.0      1.0      0.0          assert self.W1 is not None, "weight matrix W1 is not initialized"
    58      1131        705.0      0.6      0.0          assert self.W2 is not None, "weight matrix W2 is not initialized"
    59
    60                                                   # foward_input
    61      1131     442362.0    391.1      0.1          h = self.W1[X.flatten(), :].T
    62                                                   # forward output
    63      1131   92728803.0  81988.3     18.0          u = np.dot(self.W2, h)
    64
    65      1131  420658761.0 371935.2     81.8          y = self.softmax(u)
    66
    67      1131       6566.0      5.8      0.0          self.cache["X"] = X
    68      1131       1079.0      1.0      0.0          self.cache["h"] = h
    69      1131     368011.0    325.4      0.1          self.cache["logits"] = u
    70
    71      1131        939.0      0.8      0.0          return y

Total time: 152.174 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/models.py
Function: backward at line 79

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    79                                               @profile
    80                                               def backward(self, grad_softmax):
    81      1131   97837203.0  86505.0     64.3          dW2 = (1 / self.cache["h"].shape[1]) * np.dot(grad_softmax, self.cache["h"].T)
    82      1131   54333614.0  48040.3     35.7          dW1 = np.dot(self.W2.T, grad_softmax)
    83      1131       2913.0      2.6      0.0          return dW1, dW2

Total time: 12.6354 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/optimizers.py
Function: step at line 22

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    22                                               @profile
    23                                               def step(self, dW1, dW2):
    24      1131    2172105.0   1920.5     17.2          self.model.W1[self.model.cache["X"].flatten(), :] -= self.learning_rate * dW1.T
    25      1131   10458501.0   9247.1     82.8          self.model.W2 -= self.learning_rate * dW2
    26
    27      1131       4840.0      4.3      0.0          self.iterations += 1

Total time: 682.92 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_numpy/utils.py
Function: train_default at line 16

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    16                                           @profile
    17                                           def train_default(
    18                                               model: SimpleWord2Vec,
    19                                               train_dataloader: DataLoader,
    20                                               criterion: CrossEntropy,
    21                                               optimizer: Optimizer,
    22                                           ) -> float:
    23         1          0.0      0.0      0.0      train_loss = 0.0
    24      1132    3060193.0   2703.4      0.4      for i, batch in enumerate(tqdm(train_dataloader)):
    25      1131       3543.0      3.1      0.0          model.train()
    26      1131       1679.0      1.5      0.0          X, y = batch["X"], batch["Y"]
    27      1131  514228115.0 454666.8     75.3          preds = model.forward(X)
    28      1131     531357.0    469.8      0.1          loss, dy = criterion(preds, y)
    29      1131  152434649.0 134778.6     22.3          dW1, dW2 = model.backward(dy)
    30      1131   12655328.0  11189.5      1.9          optimizer.step(dW1, dW2)
    31      1131       2892.0      2.6      0.0          train_loss += loss
    32
    33      1131       2200.0      1.9      0.0          if i % 1500 == 0:
    34         1        108.0    108.0      0.0              print("Current Training Loss {:.6}".format(loss))
    35
    36         1          9.0      9.0      0.0      train_loss /= len(train_dataloader)
    37         1          0.0      0.0      0.0      return train_loss

Total time: 682.931 s
File: speed_tests/numpy_ce.py
Function: train_wrapper at line 55

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    55                                           @profile
    56                                           def train_wrapper(epochs, model, train_dataloader, criterion, optimizer):
    57         2       1554.0    777.0      0.0      for epoch in tqdm(range(epochs)):
    58         1          5.0      5.0      0.0          print(f"###################### EPOCH {epoch} ###########################")
    59         1  682929329.0 682929329.0    100.0          train_loss = train_default(model, train_dataloader, criterion, optimizer)
    60         1         43.0     43.0      0.0          print("Training loss:", train_loss)
