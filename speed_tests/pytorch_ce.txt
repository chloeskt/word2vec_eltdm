Timer unit: 1e-06 s

Total time: 32.5108 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/models.py
Function: forward at line 27

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    27                                               @profile
    28                                               def forward(self, x):
    29      1131     179071.0    158.3      0.6          x = self.embedding(x)
    30      1131   24407969.0  21580.9     75.1          x = self.linear(x)
    31      1131    7921860.0   7004.3     24.4          out = F.softmax(x, dim=1)
    32      1131       1881.0      1.7      0.0          return out

Total time: 166.401 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/utils.py
Function: train_default at line 11

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    11                                           @profile
    12                                           def train_default(
    13                                               model: PytorchSimpleWord2Vec,
    14                                               train_dataloader: DataLoader,
    15                                               criterion: nn.CrossEntropyLoss,
    16                                               optimizer: optim.Optimizer,
    17                                               device: str = "cpu",
    18                                           ) -> float:
    19         1          1.0      1.0      0.0      train_loss = 0.0
    20         1      12072.0  12072.0      0.0      model.to(device)
    21      1132    1781789.0   1574.0      1.1      for i, batch in enumerate(tqdm(train_dataloader)):
    22      1131      35685.0     31.6      0.0          model.train()
    23      1131    2449971.0   2166.2      1.5          optimizer.zero_grad()
    24      1131       6462.0      5.7      0.0          X, y = batch["X"].squeeze(), batch["Y"].squeeze()
    25      1131      38300.0     33.9      0.0          X, y = torch.tensor(X).to(device), torch.LongTensor(y).to(device)
    26      1131   35020794.0  30964.5     21.0          preds = model.forward(X)
    27      1131   13132992.0  11611.8      7.9          loss = criterion(preds, y)
    28
    29      1131   87495662.0  77361.3     52.6          loss.backward()
    30      1131   26412754.0  23353.5     15.9          optimizer.step()
    31
    32      1131      12819.0     11.3      0.0          train_loss += loss
    33
    34      1131       1183.0      1.0      0.0          if i % 1500 == 0:
    35         1         43.0     43.0      0.0              print("Current Training Loss {:.6}".format(loss))
    36
    37         1         47.0     47.0      0.0      train_loss /= len(train_dataloader)
    38         1          1.0      1.0      0.0      return train_loss

Total time: 166.413 s
File: speed_tests/pytorch_ce.py
Function: train_wrapper at line 13

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    13                                           @profile
    14                                           def train_wrapper(epochs, model, train_dataloader, criterion, optimizer):
    15         2       1159.0    579.5      0.0      for epoch in tqdm(range(epochs)):
    16         1          5.0      5.0      0.0          print(f"###################### EPOCH {epoch} ###########################")
    17         1  166411391.0 166411391.0    100.0          train_loss = train_default(model, train_dataloader, criterion, optimizer)
    18         1         14.0     14.0      0.0          print("Training loss:", train_loss.item())
