Timer unit: 1e-06 s

Total time: 0.1606 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/losses.py
Function: forward at line 9

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     9                                               @staticmethod
    10                                               @profile
    11                                               def forward(
    12                                                   input_vector: torch.Tensor,
    13                                                   output_vector: torch.Tensor,
    14                                                   noise_vectors: torch.Tensor,
    15                                               ) -> float:
    16      1131       1911.0      1.7      1.2          batch_size, embed_size = input_vector.shape
    17
    18      1131       3526.0      3.1      2.2          input_vector = input_vector.view(batch_size, embed_size, 1)
    19
    20      1131       2015.0      1.8      1.3          output_vector = output_vector.view(batch_size, 1, embed_size)
    21
    22      1131      52454.0     46.4     32.7          out_loss = torch.bmm(output_vector, input_vector).sigmoid().log()
    23      1131       5514.0      4.9      3.4          out_loss = out_loss.squeeze()
    24
    25      1131      43267.0     38.3     26.9          noise_loss = torch.bmm(noise_vectors.neg(), input_vector).sigmoid().log()
    26      1131      19025.0     16.8     11.8          noise_loss = noise_loss.squeeze().sum(1)
    27
    28      1131      32888.0     29.1     20.5          return -(out_loss + noise_loss).mean()

Total time: 0.03583 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/models.py
Function: forward_input at line 78

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    78                                               @profile
    79                                               def forward_input(self, x: torch.Tensor) -> torch.Tensor:
    80      1131      35311.0     31.2     98.6          x = self.embedding_input(x)
    81      1131        519.0      0.5      1.4          return x

Total time: 0.024645 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/models.py
Function: forward_output at line 83

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    83                                               @profile
    84                                               def forward_output(self, y: torch.Tensor) -> torch.Tensor:
    85      1131      24158.0     21.4     98.0          out = self.embedding_output(y)
    86      1131        487.0      0.4      2.0          return out

Total time: 0.661256 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/models.py
Function: forward_noise at line 88

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    88                                               @profile
    89                                               def forward_noise(self, batch_size: int, n_samples: int) -> torch.Tensor:
    90      1131        610.0      0.5      0.1          if self.noise_dist is None:
    91                                                       noise_dist = torch.ones(self.vocab_size)
    92                                                   else:
    93      1131        357.0      0.3      0.1              noise_dist = self.noise_dist
    94
    95      1131      18781.0     16.6      2.8          noise_dist = torch.tensor(noise_dist)
    96      2262     570843.0    252.4     86.3          noise_words = torch.multinomial(
    97      1131        490.0      0.4      0.1              noise_dist, batch_size * n_samples, replacement=True
    98                                                   )
    99
   100      1131      31592.0     27.9      4.8          noise_words = noise_words.to(self.device)
   101      2262      37491.0     16.6      5.7          noise_vector = self.embedding_output(noise_words).view(
   102      1131        779.0      0.7      0.1              batch_size, n_samples, self.embedding_dim
   103                                                   )
   104      1131        313.0      0.3      0.0          return noise_vector

Total time: 3.59662 s
File: /home/kaliayev/Documents/ENSAE/elements_logiciels/word2vec_eltdm/word2vec_eltdm/word2vec_accelerated/utils.py
Function: train_NSL at line 40

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    40                                           @profile
    41                                           def train_NSL(
    42                                               model: PytorchNegWord2Vec,
    43                                               train_dataloader: DataLoader,
    44                                               criterion: NegativeSamplingLoss,
    45                                               optimizer: torch.optim.Optimizer,
    46                                               n_samples: int,
    47                                               device: str = "cpu",
    48                                           ) -> float:
    49         1          0.0      0.0      0.0      train_loss = 0.0
    50         1         91.0     91.0      0.0      model.to(device)
    51      1132    1294891.0   1143.9     36.0      for i, batch in enumerate(tqdm(train_dataloader)):
    52      1131      25180.0     22.3      0.7          model.train()
    53      1131      51402.0     45.4      1.4          optimizer.zero_grad()
    54
    55      1131       4790.0      4.2      0.1          X, y = batch["X"].squeeze(), batch["Y"].squeeze()
    56      1131      11706.0     10.4      0.3          X, y = torch.LongTensor(X), torch.LongTensor(y)
    57      1131     347158.0    306.9      9.7          X, y = X.to(device), y.to(device)
    58
    59      1131      42810.0     37.9      1.2          h = model.forward_input(X)
    60      1131      29624.0     26.2      0.8          u = model.forward_output(y)
    61      1131     673081.0    595.1     18.7          noise_vector = model.forward_noise(X.shape[0], n_samples)
    62
    63                                                   # negative sampling loss
    64      1131     175143.0    154.9      4.9          loss = criterion(h, u, noise_vector)
    65      1131     719171.0    635.9     20.0          loss.backward()
    66      1131     207873.0    183.8      5.8          optimizer.step()
    67
    68      1131      11859.0     10.5      0.3          train_loss += loss
    69
    70      1131       1032.0      0.9      0.0          if i % 1500 == 0:
    71         1        735.0    735.0      0.0              print("Current Training Loss {:.6}".format(loss))
    72
    73         1         70.0     70.0      0.0      train_loss /= len(train_dataloader)
    74         1          0.0      0.0      0.0      return train_loss

Total time: 3.60702 s
File: speed_tests/pytorch_nsl.py
Function: train_wrapper at line 62

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    62                                           @profile
    63                                           def train_wrapper(epochs, model, train_dataloader, criterion, optimizer, n_samples, device):
    64         2       1150.0    575.0      0.0      for epoch in tqdm(range(epochs)):
    65         1          5.0      5.0      0.0          print(f"###################### EPOCH {epoch} ###########################")
    66         1    3604765.0 3604765.0     99.9          train_loss = train_NSL(model, train_dataloader, criterion, optimizer, n_samples, device)
    67         1       1103.0   1103.0      0.0          print("Training loss:", train_loss.item())
